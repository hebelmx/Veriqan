"""
Performance tests for the extraction pipeline.
"""

import pytest
import time
import psutil
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from unittest.mock import patch, Mock
import torch
from PIL import Image
import numpy as np

from src.api import PrismaExtractorAPI
from src.factory import ExtractorFactory
from src.modules import PerformanceMonitor


class TestPerformanceBaseline:
    """Baseline performance tests for individual components."""
    
    @pytest.mark.performance
    def test_image_loading_performance(self, create_test_images, save_test_image, benchmark_config):
        \"\"\"Test image loading performance.\"\"\"\n        from src.modules.image_processor import ImageProcessor\n        \n        processor = ImageProcessor()\n        \n        # Create test images of different sizes\n        image_sizes = [\n            (800, 600),\n            (1200, 900),\n            (1920, 1080),\n            (2560, 1440)\n        ]\n        \n        image_paths = []\n        for width, height in image_sizes:\n            img = create_test_images(width=width, height=height, add_text_patterns=True)\n            path = save_test_image(img, f'perf_{width}x{height}.png')\n            image_paths.append(str(path))\n        \n        # Benchmark image loading\n        results = []\n        \n        for path in image_paths:\n            start_time = time.time()\n            \n            try:\n                image = processor.load_image(path)\n                load_time = time.time() - start_time\n                \n                results.append({\n                    'path': path,\n                    'size': image.size,\n                    'load_time': load_time,\n                    'success': True\n                })\n                \n                # Verify reasonable loading time (< 1 second for reasonable sizes)\n                if image.size[0] * image.size[1] < 2000000:  # < 2MP\n                    assert load_time < 1.0, f\"Image loading too slow: {load_time:.3f}s for {image.size}\"\n                    \n            except Exception as e:\n                results.append({\n                    'path': path,\n                    'load_time': time.time() - start_time,\n                    'success': False,\n                    'error': str(e)\n                })\n        \n        # Verify all images loaded successfully\n        successful = [r for r in results if r['success']]\n        assert len(successful) == len(image_sizes)\n        \n        # Print performance summary\n        for result in successful:\n            print(f\"Loaded {result['size']} in {result['load_time']:.3f}s\")\n    \n    @pytest.mark.performance\n    def test_json_parsing_performance(self, sample_json_responses):\n        \"\"\"Test JSON parsing performance.\"\"\"\n        from src.modules.json_parser import JsonParser\n        \n        parser = JsonParser()\n        \n        # Test with different JSON complexities\n        test_cases = [\n            ('simple', sample_json_responses['valid']),\n            ('markdown', sample_json_responses['with_markdown']),\n            ('complex', sample_json_responses['complex']),\n            ('malformed', sample_json_responses['malformed']),\n        ]\n        \n        # Add large JSON for stress testing\n        large_json = '{' + ', '.join([f'\"key_{i}\": \"value_{i}\"' for i in range(1000)]) + '}'\n        test_cases.append(('large', large_json))\n        \n        results = []\n        \n        for test_name, json_text in test_cases:\n            # Parse multiple times for average\n            times = []\n            for _ in range(10):\n                start_time = time.time()\n                try:\n                    result = parser.parse(json_text)\n                    parse_time = time.time() - start_time\n                    times.append(parse_time)\n                    assert isinstance(result, dict)\n                except Exception as e:\n                    parse_time = time.time() - start_time\n                    times.append(parse_time)\n                    # Some parsing failures are expected for malformed JSON\n            \n            avg_time = sum(times) / len(times)\n            results.append({\n                'test_name': test_name,\n                'avg_time': avg_time,\n                'max_time': max(times),\n                'min_time': min(times)\n            })\n            \n            # Verify reasonable parsing time\n            assert avg_time < 0.1, f\"JSON parsing too slow for {test_name}: {avg_time:.3f}s\"\n        \n        # Print performance summary\n        for result in results:\n            print(f\"{result['test_name']}: {result['avg_time']:.4f}s avg, {result['max_time']:.4f}s max\")\n    \n    @pytest.mark.performance\n    def test_document_validation_performance(self, create_test_documents):\n        \"\"\"Test document validation performance.\"\"\"\n        from src.modules.document_validator import DocumentValidator\n        from src.models import Requerimiento\n        \n        validator = DocumentValidator(schema=Requerimiento)\n        \n        # Create different document complexities\n        test_documents = []\n        \n        # Simple document\n        simple_doc = create_test_documents()\n        test_documents.append(('simple', simple_doc))\n        \n        # Complex document with many fields\n        complex_doc = create_test_documents(\n            partes=[f'Parte {i}' for i in range(50)],\n            detalle={\n                'descripcion': 'Complex case with many elements',\n                'monto': 1000000.0,\n                'moneda': 'MXN',\n                'elementos': [{\n                    'id': i,\n                    'descripcion': f'Element {i}',\n                    'valor': i * 100.0\n                } for i in range(100)]\n            }\n        )\n        test_documents.append(('complex', complex_doc))\n        \n        results = []\n        \n        for doc_name, document in test_documents:\n            # Validate multiple times\n            times = []\n            for _ in range(50):  # More iterations for small operations\n                start_time = time.time()\n                is_valid, validated_data, errors = validator.validate(document)\n                validation_time = time.time() - start_time\n                times.append(validation_time)\n                \n                # Simple doc should always be valid\n                if doc_name == 'simple':\n                    assert is_valid is True\n            \n            avg_time = sum(times) / len(times)\n            results.append({\n                'doc_name': doc_name,\n                'avg_time': avg_time,\n                'max_time': max(times)\n            })\n            \n            # Verify reasonable validation time\n            assert avg_time < 0.01, f\"Document validation too slow for {doc_name}: {avg_time:.4f}s\"\n        \n        # Print performance summary\n        for result in results:\n            print(f\"{result['doc_name']}: {result['avg_time']:.4f}s avg validation\")\n    \n    @pytest.mark.performance\n    def test_performance_monitor_overhead(self):\n        \"\"\"Test performance monitor overhead.\"\"\"\n        monitor = PerformanceMonitor()\n        \n        # Test operation tracking overhead\n        operation_times = []\n        \n        # Without monitoring\n        for _ in range(1000):\n            start = time.time()\n            # Simulate work\n            sum(range(100))\n            operation_times.append(time.time() - start)\n        \n        baseline_avg = sum(operation_times) / len(operation_times)\n        \n        # With monitoring\n        monitored_times = []\n        for i in range(1000):\n            with monitor.track_operation(f\"test_op_{i}\"):\n                start = time.time()\n                # Same simulated work\n                sum(range(100))\n                monitored_times.append(time.time() - start)\n        \n        monitored_avg = sum(monitored_times) / len(monitored_times)\n        \n        # Monitor overhead should be minimal (< 10% increase)\n        overhead_ratio = monitored_avg / baseline_avg\n        assert overhead_ratio < 1.1, f\"Performance monitor overhead too high: {overhead_ratio:.2f}x\"\n        \n        print(f\"Baseline: {baseline_avg:.6f}s, Monitored: {monitored_avg:.6f}s, Overhead: {overhead_ratio:.2f}x\")\n        \n        # Verify metrics were collected\n        report = monitor.get_report()\n        assert report['summary']['total_operations'] == 1000\n\n\nclass TestLoadTesting:\n    \"\"\"Load testing scenarios.\"\"\"\n    \n    @pytest.mark.performance\n    @pytest.mark.slow\n    @pytest.mark.mock\n    def test_concurrent_extraction_load(self, create_test_images, save_test_image):\n        \"\"\"Test concurrent extraction load.\"\"\"\n        with patch('src.modules.model_loader.ModelLoader.load_model') as mock_load_model, \\\n             patch('src.modules.model_loader.ModelLoader.load_processor') as mock_load_processor, \\\n             patch('src.utils.device_utils.get_optimal_device_config') as mock_device_config:\n            \n            # Setup fast mocks\n            mock_device_config.return_value = {'device': 'cpu', 'dtype': torch.float32}\n            mock_model = Mock()\n            mock_processor = Mock()\n            \n            def fast_generate(*args, **kwargs):\n                # Simulate fast processing\n                time.sleep(0.01)\n                return torch.tensor([[1, 2, 3]])\n            \n            mock_model.generate.side_effect = fast_generate\n            mock_processor.apply_chat_template.return_value = {\n                'input_ids': torch.tensor([[1]]), \n                'attention_mask': torch.tensor([[1]])\n            }\n            mock_processor.batch_decode.return_value = ['{\"fecha\": \"2024-01-15\"}']\n            \n            mock_load_model.return_value = mock_model\n            mock_load_processor.return_value = mock_processor\n            \n            # Create test images\n            num_images = 20\n            image_paths = []\n            for i in range(num_images):\n                img = create_test_images(width=400, height=300)\n                path = save_test_image(img, f'load_test_{i}.png')\n                image_paths.append(str(path))\n            \n            # Test with different concurrency levels\n            concurrency_levels = [1, 2, 4, 8]\n            \n            for num_workers in concurrency_levels:\n                print(f\"\\nTesting with {num_workers} concurrent workers\")\n                \n                start_time = time.time()\n                successful_extractions = 0\n                \n                def extract_image(image_path):\n                    with PrismaExtractorAPI() as api:\n                        result = api.extract(image_path, 'smolvlm')\n                        return result['success']\n                \n                # Use ThreadPoolExecutor for concurrent processing\n                with ThreadPoolExecutor(max_workers=num_workers) as executor:\n                    future_to_path = {executor.submit(extract_image, path): path for path in image_paths}\n                    \n                    for future in as_completed(future_to_path):\n                        try:\n                            success = future.result()\n                            if success:\n                                successful_extractions += 1\n                        except Exception as e:\n                            print(f\"Extraction failed: {e}\")\n                \n                total_time = time.time() - start_time\n                throughput = successful_extractions / total_time\n                \n                print(f\"Workers: {num_workers}, Success: {successful_extractions}/{num_images}, \"\n                      f\"Time: {total_time:.2f}s, Throughput: {throughput:.2f} docs/sec\")\n                \n                # Verify most extractions succeeded\n                success_rate = successful_extractions / num_images\n                assert success_rate > 0.8, f\"Low success rate with {num_workers} workers: {success_rate:.2f}\"\n    \n    @pytest.mark.performance\n    @pytest.mark.slow\n    @pytest.mark.mock\n    def test_memory_usage_under_load(self, create_test_images, save_test_image):\n        \"\"\"Test memory usage under load.\"\"\"\n        with patch('src.modules.model_loader.ModelLoader.load_model') as mock_load_model, \\\n             patch('src.modules.model_loader.ModelLoader.load_processor') as mock_load_processor, \\\n             patch('src.utils.device_utils.get_optimal_device_config') as mock_device_config:\n            \n            # Setup mocks\n            mock_device_config.return_value = {'device': 'cpu', 'dtype': torch.float32}\n            mock_model = Mock()\n            mock_processor = Mock()\n            mock_model.generate.return_value = torch.tensor([[1, 2, 3]])\n            mock_processor.apply_chat_template.return_value = {\n                'input_ids': torch.tensor([[1]]), 'attention_mask': torch.tensor([[1]])\n            }\n            mock_processor.batch_decode.return_value = ['{\"fecha\": \"2024-01-15\"}']\n            mock_load_model.return_value = mock_model\n            mock_load_processor.return_value = mock_processor\n            \n            # Get initial memory usage\n            process = psutil.Process(os.getpid())\n            initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n            \n            print(f\"Initial memory usage: {initial_memory:.1f} MB\")\n            \n            # Create test images\n            num_images = 50\n            image_paths = []\n            for i in range(num_images):\n                img = create_test_images(width=600, height=450)\n                path = save_test_image(img, f'memory_test_{i}.png')\n                image_paths.append(str(path))\n            \n            memory_measurements = []\n            \n            with PrismaExtractorAPI() as api:\n                for i, image_path in enumerate(image_paths):\n                    # Extract\n                    result = api.extract(image_path, 'smolvlm')\n                    assert result['success'] is True\n                    \n                    # Measure memory every 10 extractions\n                    if i % 10 == 0:\n                        current_memory = process.memory_info().rss / 1024 / 1024\n                        memory_measurements.append(current_memory)\n                        print(f\"After {i+1} extractions: {current_memory:.1f} MB\")\n            \n            final_memory = process.memory_info().rss / 1024 / 1024\n            memory_growth = final_memory - initial_memory\n            \n            print(f\"Final memory usage: {final_memory:.1f} MB\")\n            print(f\"Memory growth: {memory_growth:.1f} MB\")\n            \n            # Memory growth should be reasonable (< 500MB for 50 extractions)\n            assert memory_growth < 500, f\"Excessive memory growth: {memory_growth:.1f} MB\"\n            \n            # Check for memory leaks (memory shouldn't grow linearly)\n            if len(memory_measurements) > 2:\n                # Calculate memory growth rate\n                early_memory = memory_measurements[1] - memory_measurements[0]\n                late_memory = memory_measurements[-1] - memory_measurements[-2] if len(memory_measurements) > 2 else 0\n                \n                # Later measurements shouldn't grow much faster than early ones\n                if early_memory > 0:\n                    growth_ratio = late_memory / early_memory\n                    assert growth_ratio < 5, f\"Possible memory leak detected: growth ratio {growth_ratio:.2f}\"\n    \n    @pytest.mark.performance\n    @pytest.mark.mock\n    def test_batch_processing_performance(self, create_test_images, save_test_image):\n        \"\"\"Test batch processing performance.\"\"\"\n        with patch('src.modules.model_loader.ModelLoader.load_model') as mock_load_model, \\\n             patch('src.modules.model_loader.ModelLoader.load_processor') as mock_load_processor, \\\n             patch('src.utils.device_utils.get_optimal_device_config') as mock_device_config:\n            \n            # Setup mocks\n            mock_device_config.return_value = {'device': 'cpu', 'dtype': torch.float32}\n            mock_model = Mock()\n            mock_processor = Mock()\n            \n            def timed_generate(*args, **kwargs):\n                time.sleep(0.05)  # Simulate processing time\n                return torch.tensor([[1, 2, 3]])\n            \n            mock_model.generate.side_effect = timed_generate\n            mock_processor.apply_chat_template.return_value = {\n                'input_ids': torch.tensor([[1]]), 'attention_mask': torch.tensor([[1]])\n            }\n            mock_processor.batch_decode.return_value = ['{\"fecha\": \"2024-01-15\"}']\n            mock_load_model.return_value = mock_model\n            mock_load_processor.return_value = mock_processor\n            \n            # Create test images\n            batch_sizes = [1, 5, 10, 20]\n            \n            for batch_size in batch_sizes:\n                print(f\"\\nTesting batch size: {batch_size}\")\n                \n                # Create images for this batch\n                image_paths = []\n                for i in range(batch_size):\n                    img = create_test_images(width=300, height=200)\n                    path = save_test_image(img, f'batch_{batch_size}_{i}.png')\n                    image_paths.append(str(path))\n                \n                with PrismaExtractorAPI() as api:\n                    # Time individual extractions\n                    start_time = time.time()\n                    individual_results = []\n                    for path in image_paths:\n                        result = api.extract(path, 'smolvlm')\n                        individual_results.append(result)\n                    individual_time = time.time() - start_time\n                    \n                    # Time batch extraction\n                    start_time = time.time()\n                    batch_results = api.extract_batch(image_paths, 'smolvlm')\n                    batch_time = time.time() - start_time\n                \n                # Verify results\n                assert len(individual_results) == batch_size\n                assert len(batch_results) == batch_size\n                \n                individual_success = sum(1 for r in individual_results if r['success'])\n                batch_success = sum(1 for r in batch_results if r['success'])\n                \n                print(f\"Individual: {individual_time:.2f}s, Batch: {batch_time:.2f}s\")\n                print(f\"Individual success: {individual_success}/{batch_size}, Batch success: {batch_success}/{batch_size}\")\n                \n                # Both should have similar success rates\n                assert individual_success == batch_success\n                \n                # For small batches, times should be similar (no significant overhead)\n                if batch_size <= 5:\n                    time_ratio = batch_time / individual_time\n                    assert 0.8 <= time_ratio <= 1.5, f\"Unexpected batch processing time ratio: {time_ratio:.2f}\"\n\n\nclass TestStressTestScenarios:\n    \"\"\"Stress testing scenarios.\"\"\"\n    \n    @pytest.mark.performance\n    @pytest.mark.slow\n    @pytest.mark.mock\n    def test_high_frequency_requests(self, create_test_images, save_test_image):\n        \"\"\"Test handling high frequency extraction requests.\"\"\"\n        with patch('src.modules.model_loader.ModelLoader.load_model') as mock_load_model, \\\n             patch('src.modules.model_loader.ModelLoader.load_processor') as mock_load_processor, \\\n             patch('src.utils.device_utils.get_optimal_device_config') as mock_device_config:\n            \n            # Setup fast mocks\n            mock_device_config.return_value = {'device': 'cpu', 'dtype': torch.float32}\n            mock_model = Mock()\n            mock_processor = Mock()\n            mock_model.generate.return_value = torch.tensor([[1, 2, 3]])\n            mock_processor.apply_chat_template.return_value = {\n                'input_ids': torch.tensor([[1]]), 'attention_mask': torch.tensor([[1]])\n            }\n            mock_processor.batch_decode.return_value = ['{\"fecha\": \"2024-01-15\"}']\n            mock_load_model.return_value = mock_model\n            mock_load_processor.return_value = mock_processor\n            \n            # Create test image\n            img = create_test_images(width=400, height=300)\n            image_path = save_test_image(img, 'stress_test.png')\n            \n            # High frequency requests\n            num_requests = 100\n            request_interval = 0.01  # 100 requests per second\n            \n            results = []\n            start_time = time.time()\n            \n            with PrismaExtractorAPI() as api:\n                for i in range(num_requests):\n                    request_start = time.time()\n                    \n                    try:\n                        result = api.extract(str(image_path), 'smolvlm')\n                        request_time = time.time() - request_start\n                        \n                        results.append({\n                            'success': result['success'],\n                            'request_time': request_time,\n                            'request_id': i\n                        })\n                        \n                    except Exception as e:\n                        results.append({\n                            'success': False,\n                            'error': str(e),\n                            'request_time': time.time() - request_start,\n                            'request_id': i\n                        })\n                    \n                    # Maintain request frequency\n                    time.sleep(max(0, request_interval - (time.time() - request_start)))\n            \n            total_time = time.time() - start_time\n            successful_requests = sum(1 for r in results if r['success'])\n            \n            print(f\"\\nStress test results:\")\n            print(f\"Total requests: {num_requests}\")\n            print(f\"Successful: {successful_requests}\")\n            print(f\"Success rate: {successful_requests/num_requests:.2%}\")\n            print(f\"Total time: {total_time:.2f}s\")\n            print(f\"Avg request time: {sum(r['request_time'] for r in results)/len(results):.3f}s\")\n            \n            # Should handle most requests successfully\n            success_rate = successful_requests / num_requests\n            assert success_rate > 0.9, f\"High frequency stress test failed: {success_rate:.2%} success rate\"\n    \n    @pytest.mark.performance\n    @pytest.mark.slow\n    def test_large_image_stress(self, temp_dir):\n        \"\"\"Test processing very large images.\"\"\"\n        from src.modules.image_processor import ImageProcessor\n        \n        processor = ImageProcessor()\n        \n        # Create large images of different sizes\n        large_sizes = [\n            (3840, 2160),   # 4K\n            (5120, 2880),   # 5K\n            (7680, 4320),   # 8K (if memory allows)\n        ]\n        \n        for width, height in large_sizes:\n            print(f\"\\nTesting {width}x{height} image\")\n            \n            try:\n                # Create large image\n                large_image = Image.new('RGB', (width, height), color='white')\n                \n                # Add some content to make it realistic\n                pixels = np.array(large_image)\n                # Add text-like patterns\n                for y in range(100, height-100, 50):\n                    for x in range(100, min(width-100, 1000), 200):\n                        pixels[y:y+20, x:x+150] = [50, 50, 50]\n                \n                large_image = Image.fromarray(pixels)\n                \n                # Save and load\n                image_path = temp_dir / f'large_{width}x{height}.png'\n                large_image.save(image_path)\n                \n                # Test loading performance\n                start_time = time.time()\n                loaded_image = processor.load_image(image_path)\n                load_time = time.time() - start_time\n                \n                print(f\"Load time: {load_time:.2f}s\")\n                print(f\"Memory size: ~{(width * height * 3) / (1024*1024):.1f} MB\")\n                \n                # Verify image loaded correctly\n                assert loaded_image.size == (width, height)\n                \n                # Test preprocessing performance\n                start_time = time.time()\n                processed_image = processor.preprocess_for_ocr(loaded_image)\n                process_time = time.time() - start_time\n                \n                print(f\"Preprocessing time: {process_time:.2f}s\")\n                \n                # Should handle large images in reasonable time\n                total_pixels = width * height\n                pixels_per_second = total_pixels / (load_time + process_time)\n                \n                print(f\"Processing rate: {pixels_per_second/1000000:.1f} Mpixels/sec\")\n                \n                # Should process at least 10M pixels per second\n                assert pixels_per_second > 10000000, f\"Large image processing too slow: {pixels_per_second:.0f} pixels/sec\"\n                \n                # Clean up\n                image_path.unlink()\n                \n            except MemoryError:\n                print(f\"Skipping {width}x{height} due to memory constraints\")\n                continue\n            except Exception as e:\n                print(f\"Error testing {width}x{height}: {e}\")\n                # Don't fail the test for expected resource limitations\n                continue\n    \n    @pytest.mark.performance\n    @pytest.mark.mock\n    def test_error_recovery_performance(self, create_test_images, save_test_image):\n        \"\"\"Test performance when handling errors.\"\"\"\n        with patch('src.modules.model_loader.ModelLoader.load_model') as mock_load_model, \\\n             patch('src.modules.model_loader.ModelLoader.load_processor') as mock_load_processor, \\\n             patch('src.utils.device_utils.get_optimal_device_config') as mock_device_config:\n            \n            # Setup mocks that fail intermittently\n            mock_device_config.return_value = {'device': 'cpu', 'dtype': torch.float32}\n            mock_model = Mock()\n            mock_processor = Mock()\n            \n            call_count = {'count': 0}\n            def intermittent_failure(*args, **kwargs):\n                call_count['count'] += 1\n                # Fail every 3rd call\n                if call_count['count'] % 3 == 0:\n                    raise RuntimeError(\"Intermittent failure\")\n                return torch.tensor([[1, 2, 3]])\n            \n            mock_model.generate.side_effect = intermittent_failure\n            mock_processor.apply_chat_template.return_value = {\n                'input_ids': torch.tensor([[1]]), 'attention_mask': torch.tensor([[1]])\n            }\n            mock_processor.batch_decode.return_value = ['{\"fecha\": \"2024-01-15\"}']\n            mock_load_model.return_value = mock_model\n            mock_load_processor.return_value = mock_processor\n            \n            # Create test image\n            img = create_test_images(width=300, height=200)\n            image_path = save_test_image(img, 'error_recovery_test.png')\n            \n            # Test error handling performance\n            num_requests = 30\n            results = []\n            \n            start_time = time.time()\n            \n            with PrismaExtractorAPI() as api:\n                for i in range(num_requests):\n                    request_start = time.time()\n                    \n                    result = api.extract(str(image_path), 'smolvlm')\n                    request_time = time.time() - request_start\n                    \n                    results.append({\n                        'success': result['success'],\n                        'request_time': request_time,\n                        'request_id': i\n                    })\n            \n            total_time = time.time() - start_time\n            \n            successful_requests = sum(1 for r in results if r['success'])\n            failed_requests = sum(1 for r in results if not r['success'])\n            \n            avg_success_time = sum(r['request_time'] for r in results if r['success']) / max(successful_requests, 1)\n            avg_failure_time = sum(r['request_time'] for r in results if not r['success']) / max(failed_requests, 1)\n            \n            print(f\"\\nError recovery performance:\")\n            print(f\"Successful: {successful_requests}, Failed: {failed_requests}\")\n            print(f\"Avg success time: {avg_success_time:.3f}s\")\n            print(f\"Avg failure time: {avg_failure_time:.3f}s\")\n            \n            # Error handling shouldn't be significantly slower than success\n            if successful_requests > 0 and failed_requests > 0:\n                time_ratio = avg_failure_time / avg_success_time\n                assert time_ratio < 2.0, f\"Error handling too slow: {time_ratio:.2f}x slower than success\"\n            \n            # Should have expected failure rate (every 3rd call)\n            expected_success_rate = 2/3\n            actual_success_rate = successful_requests / num_requests\n            assert abs(actual_success_rate - expected_success_rate) < 0.15, \\\n                f\"Unexpected success rate: {actual_success_rate:.2f} vs expected {expected_success_rate:.2f}\"