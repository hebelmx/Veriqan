"""Integration tests for document generation pipeline."""

import pytest
import os
import shutil
from pathlib import Path
from unittest.mock import patch, Mock
import json

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from generate_documents import run_generation
import argparse


class TestDocumentGeneration:
    """Integration tests for the full generation pipeline."""

    @pytest.fixture
    def test_output_dir(self, tmp_path):
        """Create a temporary output directory for tests."""
        output_dir = tmp_path / "test_output"
        output_dir.mkdir()
        yield output_dir
        # Cleanup
        if output_dir.exists():
            shutil.rmtree(output_dir)

    @pytest.fixture
    def mock_args(self, test_output_dir):
        """Create mock arguments for testing."""
        args = argparse.Namespace()
        args.num = 2
        args.output = str(test_output_dir / "corpus.json")
        args.seed = 42
        args.ollama_model = "llama3"
        args.ollama_url = "http://localhost:11434"
        args.ollama_port = "11434"
        args.skip_orchestration = True  # Skip Docker for tests
        args.container_name = "ollama"
        args.use_gpu = False
        args.skip_prewarm = True
        args.fixtures_output = str(test_output_dir / "fixtures")
        args.fixtures_format = "pdf"
        args.audit_log = None
        args.batch = None
        args.prp1_summary = None
        args.allow_fallback = True
        args.continue_on_error = False
        args.debug = False
        args.debug_prompts = False
        return args

    @patch('prp1_generator.ollama_client.requests.post')
    def test_generation_with_llm_success(self, mock_post, mock_args, test_output_dir):
        """Test successful document generation using LLM."""
        # Mock successful LLM response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.iter_lines.return_value = [
            json.dumps({"response": "Test legal text ", "done": False}),
            json.dumps({"response": "generated by LLM.", "done": True}),
        ]
        mock_post.return_value = mock_response

        # Run generation
        run_generation(mock_args)

        # Verify corpus was created
        corpus_path = Path(mock_args.output)
        assert corpus_path.exists()

        # Verify corpus contents
        with open(corpus_path, 'r', encoding='utf-8') as f:
            corpus = json.load(f)

        assert len(corpus) == 2
        assert corpus[0]['record_id'] == "REQ0001"
        assert corpus[1]['record_id'] == "REQ0002"
        assert "Test legal text" in corpus[0]['text']

    @patch('prp1_generator.ollama_client.requests.post')
    def test_generation_with_fallback(self, mock_post, mock_args, test_output_dir):
        """Test document generation falls back to templates when LLM fails."""
        # Mock LLM failure
        mock_post.side_effect = Exception("Connection refused")

        # Run generation
        run_generation(mock_args)

        # Verify corpus was created with fallback content
        corpus_path = Path(mock_args.output)
        assert corpus_path.exists()

        with open(corpus_path, 'r', encoding='utf-8') as f:
            corpus = json.load(f)

        assert len(corpus) == 2
        # Fallback templates should still produce valid content
        assert corpus[0]['text'] != ""

    def test_generation_creates_fixtures(self, mock_args, test_output_dir):
        """Test that fixture files are created."""
        mock_args.skip_orchestration = True
        mock_args.allow_fallback = True

        with patch('prp1_generator.ollama_client.requests.post') as mock_post:
            mock_post.side_effect = Exception("Force fallback")
            run_generation(mock_args)

        # Check that fixture directory was created
        fixtures_dir = Path(mock_args.fixtures_output)
        assert fixtures_dir.exists()

        # Check for generated files
        generated_files = list(fixtures_dir.glob("REQ*"))
        assert len(generated_files) > 0

    @patch('prp1_generator.ollama_client.requests.post')
    def test_generation_with_seed_reproducibility(self, mock_post, mock_args, test_output_dir):
        """Test that same seed produces reproducible results."""
        # Mock LLM response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.iter_lines.return_value = [
            json.dumps({"response": "Consistent text", "done": True}),
        ]
        mock_post.return_value = mock_response

        # Generate first batch
        run_generation(mock_args)
        with open(mock_args.output, 'r', encoding='utf-8') as f:
            corpus1 = json.load(f)

        # Generate second batch with same seed
        run_generation(mock_args)
        with open(mock_args.output, 'r', encoding='utf-8') as f:
            corpus2 = json.load(f)

        # Metadata should be reproducible (same seed)
        assert corpus1[0]['metadata']['expediente'] == corpus2[0]['metadata']['expediente']
        assert corpus1[0]['metadata']['autoridadEmisora'] == corpus2[0]['metadata']['autoridadEmisora']

    @patch('prp1_generator.ollama_client.requests.post')
    def test_generation_continues_on_error(self, mock_post, mock_args, test_output_dir):
        """Test that generation continues when continue_on_error is enabled."""
        mock_args.continue_on_error = True
        mock_args.num = 3

        # Make second generation fail
        call_count = [0]
        def side_effect(*args, **kwargs):
            call_count[0] += 1
            if call_count[0] == 2:
                raise Exception("Simulated error")
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.iter_lines.return_value = [
                json.dumps({"response": "Success", "done": True}),
            ]
            return mock_response

        mock_post.side_effect = side_effect

        # Should complete with 2 out of 3 records (one failed)
        run_generation(mock_args)

        corpus_path = Path(mock_args.output)
        assert corpus_path.exists()

        with open(corpus_path, 'r', encoding='utf-8') as f:
            corpus = json.load(f)

        # Should have 2 successful records despite 3 attempts
        assert len(corpus) == 2


class TestFixtureGeneration:
    """Test fixture file generation."""

    @pytest.fixture
    def sample_metadata(self):
        """Sample metadata for fixture generation."""
        return {
            "fecha": "2024-01-15",
            "autoridadEmisora": "Tribunal Superior de Justicia",
            "expediente": "EXP-2024-001",
            "tipoRequerimiento": "Información Financiera",
            "subtipoRequerimiento": "Cuentas de Cheques",
            "fundamentoLegal": "Artículo 115 LIC",
            "motivacion": "Juicio Ordinario Mercantil",
            "partes": ["Juan Pérez López", "Empresa XYZ S.A."],
            "detalle": {
                "descripcion": "Se requiere información de cuentas bancarias",
                "monto": None,
                "moneda": "MXN"
            },
            "plazoDias": 5,
            "plazoDescripcion": "Plazo de 5 días hábiles"
        }

    def test_pdf_generation(self, sample_metadata, tmp_path):
        """Test PDF fixture generation."""
        from prp1_generator import FixtureRenderer

        renderer = FixtureRenderer(tmp_path, fmt="pdf", seed=42)
        sections = {
            "autoridad": "Test authority text",
            "instrucciones": "Test instructions",
            "apercibimiento": "Test warning",
            "firmas": "Test signatures"
        }

        artifacts = renderer.render("TEST001", sample_metadata, sections)

        assert "pdf" in artifacts
        pdf_path = Path(artifacts["pdf"])
        assert pdf_path.exists()
        assert pdf_path.suffix == ".pdf"

    def test_docx_generation(self, sample_metadata, tmp_path):
        """Test DOCX fixture generation."""
        from prp1_generator import FixtureRenderer

        renderer = FixtureRenderer(tmp_path, fmt="pdf", seed=42)
        sections = {
            "autoridad": "Test authority text",
            "instrucciones": "Test instructions",
            "apercibimiento": "Test warning",
            "firmas": "Test signatures"
        }

        artifacts = renderer.render("TEST001", sample_metadata, sections)

        assert "docx" in artifacts
        docx_path = Path(artifacts["docx"])
        assert docx_path.exists()
        assert docx_path.suffix == ".docx"

    def test_xml_generation(self, sample_metadata, tmp_path):
        """Test XML fixture generation."""
        from prp1_generator import FixtureRenderer
        import xml.etree.ElementTree as ET

        renderer = FixtureRenderer(tmp_path, fmt="pdf", seed=42)
        sections = {
            "autoridad": "Test authority text",
            "instrucciones": "Test instructions",
            "apercibimiento": "Test warning",
            "firmas": "Test signatures"
        }

        artifacts = renderer.render("TEST001", sample_metadata, sections)

        assert "xml" in artifacts
        xml_path = Path(artifacts["xml"])
        assert xml_path.exists()

        # Verify XML is well-formed
        tree = ET.parse(xml_path)
        root = tree.getroot()
        assert root.tag == "Requerimiento"
