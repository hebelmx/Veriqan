# Post-Funding Enhancement Proposal for ExxerCube.Prisma

## 1. Introduction

This document outlines the strategic, multi-phase roadmap for the enhancement of the ExxerCube.Prisma application after initial funding is secured. The plan prioritizes foundational data quality and optimization first, followed by the implementation of robust architectural patterns and high-value client features. This approach ensures that the core data processing pipeline is highly reliable before expanding the system's functional scope.

---

## 2. Phase 1: Foundational Data Quality Optimization

**Objective:** Maximize the accuracy and reliability of the core data extraction and reconciliation pipeline to build a trustworthy foundation for all subsequent processing.

### Task 1.1: Fusion Strategy Optimization

-   **Description:** Systematically tune the coefficients and weights within the `FusionExpedienteService`. This service is responsible for merging data from multiple sources (XML, OCR, DOCX) into a single, coherent record.
-   **Methodology:** This task will replicate the successful, data-driven experimental approach used for image filter optimization, as documented in `OCR_FILTER_OPTIMIZATION_JOURNAL.md`. A series of experiments will be run with a curated dataset to find the optimal values for fuzzy matching thresholds, source reliability scores, and weighted voting parameters.
-   **Goal:** Increase the accuracy of automated data reconciliation, minimize incorrect fusions, and reduce the number of fields flagged for manual review due to conflicts.

### Task 1.2: OCR Image Filter Enhancement (Two Rounds)

-   **Description:** Conduct two additional rounds of focused optimization on the image enhancement pipeline used for OCR pre-processing.
-   **Methodology:** Leveraging the findings and methodology from `OCR_FILTER_OPTIMIZATION_JOURNAL.md`, these rounds will explore new filter combinations and fine-tune existing filter parameters to further improve text extraction from low-quality or degraded documents.
-   **Goal:** Increase the raw accuracy of the OCR output, which is a primary source of data errors and a critical input for the data fusion service.

---

## 3. Phase 2: Architectural Enhancements & Production-Grade Logic

**Objective:** Replace temporary placeholder components with robust, scalable, and production-ready services, and introduce new architectural patterns to support future growth.

### Task 2.1: Implement the `IEntityCatalog` Service Pattern

-   **Description:** Introduce a new architectural abstraction, `IEntityCatalog`, for the purpose of validating and standardizing extracted entities against official dictionaries.
-   **Methodology:**
    1.  Define a generic `IEntityCatalog` interface in the Domain layer.
    2.  Create concrete implementations in the Infrastructure layer for different entity types (e.g., `SatAuthorityCatalog`, `FgrFiscaliaCatalog`).
    3.  These services will be populated from two primary sources:
        -   **Public Data:** Dictionaries created by data-mining the public documents referenced in `ENTITY_SOURCES_CATALOG.md`.
        -   **LLM Distillation:** Vocabularies of technical and legal jargon generated by specialized, fine-tuned local LLM agents (via Ollama).
    4.  Inject these catalog services into the relevant extractor components.
-   **Goal:** Dramatically improve the accuracy of key entity extractions (e.g., standardizing authority names). This provides a clean, decoupled, and scalable pattern for adding new dictionaries as the system evolves.

### Task 2.2: Implement the "Dictionary Comparer" for Directive Classification

-   **Description:** Replace the current naive, keyword-based `LegalDirectiveClassifierService` with a robust, deterministic "dictionary comparer".
-   **Methodology:** The refactored service will leverage the `IEntityCatalog` from Task 2.1 (specifically for legal jargon) and the existing fuzzy matching engine (`ITextComparer`). It will compare phrases from the document against a dictionary of legal actions to accurately determine the document's intent (Block, Unblock, etc.) and populate the `Expediente.SemanticAnalysis` model.
-   **Goal:** To fix the primary blocker to full automation. This will ensure reliable and deterministic classification of compliance actions, moving the system from a prototype to a trustworthy production engine.

---

## 4. Phase 3: High-Value Feature Development

**Objective:** Address major client-facing features and deliverables required for full compliance and operational value.

### Task 3.1: R29 Report Generation (Analysis & Proposal)

-   **Description:** Formally analyze and propose the implementation of the complete R29 report generation feature.
-   **Methodology:**
    1.  Conduct a definitive gap analysis of the `Expediente` and `LawMandatedFields` models against the 42 mandatory fields outlined in the R29 specification.
    2.  Identify all fields that must be sourced from the bank's internal systems (e.g., `BranchCode`, `ProductType`, `FinalBalance`).
    3.  Design the integration points and data contracts required to retrieve this information from the bank's core systems.
    4.  The initial requirements for this feature are summarized in `docs/Legal/LegalRequirements_Summary.md`.
-   **Goal:** To deliver a comprehensive technical design, project plan, and effort estimation for the implementation of this critical compliance deliverable. This proposal will form the basis for the final stage of the project.

### Task 3.2: Address Further Client-Requested Enhancements

-   **Description:** This task will encompass any additional high-value features or enhancements requested by the client after the foundational and architectural improvements from Phases 1 and 2 are complete.
-   **Goal:** To remain agile and responsive to evolving client needs once the core system is stable and reliable.
